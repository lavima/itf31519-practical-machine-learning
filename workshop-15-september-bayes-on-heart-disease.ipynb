{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/larsmagnusson/workshop-15-september-bayes-on-heart-disease?scriptVersionId=143534772\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-19T13:58:23.756881Z","iopub.execute_input":"2023-09-19T13:58:23.758018Z","iopub.status.idle":"2023-09-19T13:58:23.777547Z","shell.execute_reply.started":"2023-09-19T13:58:23.757979Z","shell.execute_reply":"2023-09-19T13:58:23.776238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset. No need to specify column names since it is part of the data\ndataset = pd.read_csv('/kaggle/input/heart-disease-cleveland-uci/heart_cleveland_upload.csv')\n\n# The name of all the features containing categorical data\ncat_names = ['sex', 'cp', 'fbs', 'restecg', 'slope', 'thal', 'exang']\nall_cat_names = ['sex', 'cp', 'fbs', 'restecg', 'slope', 'thal', 'exang', 'condition']\n\n# Convert all categorical features to appropriate type\ndataset[all_cat_names] = dataset[all_cat_names].astype('category')\n\n# Split into 75/25 train/test (the training data will be used for cross validation)\ntrain_data = dataset.groupby('condition', group_keys=False).apply(lambda x: x.sample(frac=0.75))\ntest_data = dataset.drop(train_data.index)\n\n# Select all the features by dropping the target column in both training and test partition\ntrain_targets = train_data['condition']\ntrain_features = train_data.drop('condition', axis=1)\ntest_targets = train_data['condition']\ntest_features = train_data.drop('condition', axis=1)\n\n# Separate categorical features from numerical features in both training and test data.\n# This is used when we try to manually solve the issues with different data types, but we \n# don't need it for our custom estimator (which handles this internally)\ncat_train_features = train_features[cat_names];\nnum_train_features = train_features.drop(cat_names, axis=1)\ncat_test_features = test_features[cat_names];\nnum_test_features = test_features.drop(cat_names, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:58:23.780236Z","iopub.execute_input":"2023-09-19T13:58:23.780664Z","iopub.status.idle":"2023-09-19T13:58:23.815506Z","shell.execute_reply.started":"2023-09-19T13:58:23.780625Z","shell.execute_reply":"2023-09-19T13:58:23.814636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.naive_bayes import CategoricalNB, GaussianNB\n\n# Create a combined pipeline using an ordinal encoder to encode the featues and a \n# categorical naive bayes classifier\ncat_classifier = make_pipeline(OrdinalEncoder(), CategoricalNB())\n\n# Create a classifier for the numerical data\nnum_classifier = GaussianNB()\n\n# Train the classifiers on their respective features\ncat_classifier.fit(cat_train_features, train_targets)\nnum_classifier.fit(num_train_features, train_targets)\n\n# Print score/accuracy of the classifiers\n(cat_classifier.score(cat_test_features, test_targets),num_classifier.score(num_test_features, test_targets))","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:58:23.81701Z","iopub.execute_input":"2023-09-19T13:58:23.817716Z","iopub.status.idle":"2023-09-19T13:58:23.845861Z","shell.execute_reply.started":"2023-09-19T13:58:23.817675Z","shell.execute_reply":"2023-09-19T13:58:23.844472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This shows how we can combine the output from two classifiers\n\n# Fetch the class probabilities\ncat_proba = cat_classifier.predict_proba(cat_test_features)\nnum_proba = num_classifier.predict_proba(num_test_features)\n\n# Combine by elementwise multiplication\nproba = cat_proba * num_proba\n\n# Print the combined probailities (one element per class per instance)\nprint(proba)\n\n# Find and print the index of the maximum probability (this should be used to access the \n# class labels)\npredictions = np.argmax(proba,axis=1)\npredictions","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:58:23.847443Z","iopub.execute_input":"2023-09-19T13:58:23.847793Z","iopub.status.idle":"2023-09-19T13:58:23.872179Z","shell.execute_reply.started":"2023-09-19T13:58:23.847765Z","shell.execute_reply":"2023-09-19T13:58:23.871182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This shows how to do a simple exhaustive grid search\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Show the tunable parameters of our chosen algorithm\n#test_classifier = GaussianNB()\n#test_classifier.get_params()\n\n# Setup which parameter, and parameter values to test. These values are\n# not necessarily ideal, but show how to specify a list of values \nparams = {'var_smoothing': [0.0001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5]} \n\n# Create the grid search object. We'll be using a gaussian naive bayes classifier and we'll tune \n# one single parameter. We'll also be using 5-fold cross validation\ngrid = GridSearchCV(GaussianNB(), params, cv=5)\n\n# Run the grid search and retrain using optimal parameter values\ngrid.fit(train_features, train_targets)\n# Show the result with statistics for each individual fold\ngrid.cv_results_\n# Test the accuracy of the optimized model on test data\n#grid.score(test_features, test_targets)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:58:23.874523Z","iopub.execute_input":"2023-09-19T13:58:23.874894Z","iopub.status.idle":"2023-09-19T13:58:24.189756Z","shell.execute_reply.started":"2023-09-19T13:58:23.874861Z","shell.execute_reply":"2023-09-19T13:58:24.188502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shows how to create a custom classifier that combines two classifiers, in our case\n# CategoricalNB and GaussianNB, but the setup will work for others\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\n\n# Define our estimator/classifier, inherits from BaseEstimator and ClassifierMixin\nclass OurNaivety(BaseEstimator, ClassifierMixin):\n    # The constructor specify all tunable parameters (accessible through get_params()).\n    def __init__(self, cat_classifier=CategoricalNB(),num_classifier=GaussianNB()):\n        self.cat_classifier= cat_classifier\n        self.num_classifier= num_classifier\n    \n    # Train our classifier\n    def fit(self,X,y):\n        #self.X = X\n        #self.y = y\n        \n        # Seperate our data into numerical and categorical data\n        cat_X = X[cat_names];\n        num_X = X.drop(cat_names, axis=1)\n        \n        # Delegate the training to our two classifiers\n        self.cat_classifier.fit(cat_X,y)\n        self.num_classifier.fit(num_X,y)\n        \n        # Store a local copy of the class labels found. We assume that both the classifiers \n        # have found the same labels and in the same order for this to work\n        self.classes_ = self.cat_classifier.classes_\n        \n        # This is to follow the convention\n        return self\n    \n    # Predict the class probabilities\n    def predict_proba(self,data):\n        # Seperate our data into numerical and categorical data\n        cat_data = data[cat_names];\n        num_data = data.drop(cat_names, axis=1)\n        \n        # Combine and return the class probabilities \n        cat_proba = self.cat_classifier.predict_proba(cat_data)\n        num_proba = self.num_classifier.predict_proba(num_data)\n        proba = cat_proba * num_proba\n        \n        return proba\n        \n    # Predict class labels    \n    def predict(self,data):\n        # Use the index of the maximum probability to access the class labels\n        return self.classes_[np.argmax(self.predict_proba(data), axis=1)]","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:58:24.191747Z","iopub.execute_input":"2023-09-19T13:58:24.192112Z","iopub.status.idle":"2023-09-19T13:58:24.203315Z","shell.execute_reply.started":"2023-09-19T13:58:24.192082Z","shell.execute_reply":"2023-09-19T13:58:24.202166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shows how to run a grid search using our combined classifier. \n\n# Test our classifier (not needed)\nmodel = OurNaivety()\nmodel.fit(train_features,train_targets)\nmodel.score(test_features, test_targets)\n\n# Get available parameters\nmodel.get_params(deep=True)\n\n# Set the grid search space\nparams2 = {\n    'cat_classifier__alpha':[0.5, 1.0],\n    'num_classifier__var_smoothing': [1e-09, 1e-05]\n          }\n# Create the grid search, using 5-fold cross validation\ngrid2 = GridSearchCV(OurNaivety(), params2, cv=5)\n\n# Train the model and tune the parameters, and test the best estimator on test data\ngrid2.fit(train_features, train_targets)\ngrid2.score(test_features, test_targets)\n#grid2.cv_results_","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:58:24.204947Z","iopub.execute_input":"2023-09-19T13:58:24.205319Z","iopub.status.idle":"2023-09-19T13:58:24.590366Z","shell.execute_reply.started":"2023-09-19T13:58:24.205288Z","shell.execute_reply":"2023-09-19T13:58:24.589218Z"},"trusted":true},"execution_count":null,"outputs":[]}]}